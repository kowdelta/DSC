{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cons_data= np.load('../high_psi dataset/x_cons_data.npy')\n",
    "hx_three_data= np.load('../high_psi dataset/x_three_data.npy')\n",
    "lx_three_data= np.load('../low_psi_data/x_three_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39128, 283, 5), (1388, 283, 5), (944, 283, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoded+conservation ( consrv was not used in our paper)\n",
    "x_cons_data.shape,hx_three_data.shape,lx_three_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_split(x_cons_data,hx_three_data,lx_three_data,s):\n",
    "    \n",
    "    a=int(x_cons_data.shape[0]/10)\n",
    "    b=int(hx_three_data.shape[0]/10)\n",
    "    c=int(lx_three_data.shape[0]/10)\n",
    "    \n",
    "    #9 folds for training\n",
    "    train=x_cons_data[:a*s]\n",
    "    train=np.concatenate((train,x_cons_data[a*(s+1):]),axis=0)\n",
    "\n",
    "    d=int((9*a)/(9*(b+c)))\n",
    "    print(d)\n",
    "    for i in range (d):\n",
    "        train=np.concatenate((train,hx_three_data[:b*s]),axis=0)\n",
    "        train=np.concatenate((train,hx_three_data[b*(s+1):]),axis=0)\n",
    "    \n",
    "        train=np.concatenate((train,lx_three_data[:c*s]),axis=0)\n",
    "        train=np.concatenate((train,lx_three_data[c*(s+1):]),axis=0)\n",
    "      \n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(train)\n",
    "    \n",
    "    # 1 fold for testing\n",
    "    \n",
    "    htest=np.concatenate((hx_three_data[b*s:b*(s+1)],x_cons_data[a*s:a*(s+1)]),axis=0)\n",
    "    lt=   np.concatenate((lx_three_data[c*s:c*(s+1)],x_cons_data[a*s:a*(s+1)]),axis=0)\n",
    "\n",
    "    test=htest\n",
    "    test=np.concatenate((test,lx_three_data[c*s:c*(s+1)]),axis=0)\n",
    "    \n",
    "    \n",
    "    return train,test,htest,lt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((42888, 283, 5), (2664, 283, 5), (2570, 283, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test,htest,ltest=cross_split(x_cons_data[:24320],hx_three_data,lx_three_data,1)\n",
    "\n",
    "y_train=train[:,l-1,0]\n",
    "y_test=test[:,l-1,0]\n",
    "hy_test=htest[:,l-1,0]\n",
    "ly_test=ltest[:,l-1,0]\n",
    "\n",
    "train.shape,test.shape,htest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_AUC():\n",
    "    ypreds=model.predict([htest[:,:l-1,:4],htest[:,-1,0:3]])\n",
    "    fpr, tpr, thresholds = roc_curve(hy_test, ypreds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ypreds2=model.predict([test[:,:l-1,:4],test[:,-1,0:3]])\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(y_test, ypreds2)\n",
    "    roc_auc2 = auc(fpr2, tpr2)\n",
    "    \n",
    "    ypreds3=model.predict([ltest[:,:l-1,:4],ltest[:,-1,0:3]])\n",
    "    fpr3, tpr3, thresholds3 = roc_curve(ly_test, ypreds3)\n",
    "    roc_auc3 = auc(fpr3, tpr3)\n",
    "    \n",
    "\n",
    "    lw=2\n",
    "    \n",
    "    \n",
    "    \n",
    "    f=plt.figure()\n",
    "    plt.plot(fpr2, tpr2, color='orange', lw=1, label=    'All ALT3 events (AUC= %0.2f)' % roc_auc2)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='red', lw=1,          label='HEvents ALT3 subset (AUC= %0.2f)' % roc_auc)\n",
    "    \n",
    "    plt.plot(fpr3, tpr3, color='navy', lw=1,       label='MREvents ALT3 subset (AUC= %0.2f)' % roc_auc3)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    f.savefig(\"../plots/3.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from keras import initializers\n",
    "from keras.layers import Activation, Dense, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 140, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 140, 32)      928         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 140, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 140, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 70, 32)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 67, 8)        1032        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 67, 8)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 67, 8)        0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 33, 8)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 31, 8)        200         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 31, 8)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 31, 8)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 15, 8)        0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 120)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 123)          0           flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           3968        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,161\n",
      "Trainable params: 6,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(l-1, 4))\n",
    "inputs3 = Input(shape=(3,))\n",
    "\n",
    "\n",
    "x=Conv1D(filters=32,kernel_size=7,strides=1,kernel_initializer=initializers.random_uniform(),padding=\"same\") (inputs1)\n",
    "x=Dropout(0.2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "x=Conv1D(filters=8,kernel_size=4,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "x=Dropout(0.2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "x=Conv1D(filters=8,kernel_size=3,strides=1,kernel_initializer=initializers.random_uniform()) (x)\n",
    "x=Dropout(0.2)(x)\n",
    "x=Activation('relu')(x)\n",
    "x=MaxPooling1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x2=Flatten()(x)\n",
    "\n",
    "\n",
    "x2=keras.layers.concatenate([x2,inputs3],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x3=Dense(32, activation='relu',)(x2)\n",
    "x3=Dropout(0.5)(x3)\n",
    "\n",
    "\n",
    "x3=Dense(1, activation='sigmoid',  )(x3)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs1,inputs3], outputs=x3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(lr=0.00005,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.999,\n",
    "                                        epsilon=1e-08,\n",
    "                                        decay=0.0),metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 11s 266us/step - loss: 0.6978 - acc: 0.5185 - val_loss: 0.6866 - val_acc: 0.5383\n",
      "AUC under ROC for high inclusion exons: 0.5004380005720823\n",
      "epoch : 1\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.6934 - acc: 0.5332 - val_loss: 0.6642 - val_acc: 0.8033\n",
      "AUC under ROC for high inclusion exons: 0.5989657823226545\n",
      "epoch : 2\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.6835 - acc: 0.5772 - val_loss: 0.6780 - val_acc: 0.5980\n",
      "AUC under ROC for high inclusion exons: 0.6205276268115942\n",
      "epoch : 3\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.6773 - acc: 0.5851 - val_loss: 0.6643 - val_acc: 0.6348\n",
      "AUC under ROC for high inclusion exons: 0.6240852641113654\n",
      "epoch : 4\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.6758 - acc: 0.5874 - val_loss: 0.6546 - val_acc: 0.6618\n",
      "AUC under ROC for high inclusion exons: 0.6299580472921433\n",
      "epoch : 5\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.6734 - acc: 0.5918 - val_loss: 0.6558 - val_acc: 0.6580\n",
      "AUC under ROC for high inclusion exons: 0.6340222158657512\n",
      "epoch : 6\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.6705 - acc: 0.5965 - val_loss: 0.6439 - val_acc: 0.6869\n",
      "AUC under ROC for high inclusion exons: 0.6375351592295957\n",
      "epoch : 7\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.6680 - acc: 0.6000 - val_loss: 0.6459 - val_acc: 0.6828\n",
      "AUC under ROC for high inclusion exons: 0.6421863081617087\n",
      "epoch : 8\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.6666 - acc: 0.6020 - val_loss: 0.6656 - val_acc: 0.6246\n",
      "AUC under ROC for high inclusion exons: 0.6452940265064835\n",
      "epoch : 9\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.6655 - acc: 0.6080 - val_loss: 0.6487 - val_acc: 0.6704\n",
      "AUC under ROC for high inclusion exons: 0.6497783180778032\n",
      "epoch : 10\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 63us/step - loss: 0.6626 - acc: 0.6102 - val_loss: 0.6578 - val_acc: 0.6438\n",
      "AUC under ROC for high inclusion exons: 0.6543400791380626\n",
      "epoch : 11\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.6605 - acc: 0.6124 - val_loss: 0.6478 - val_acc: 0.6719\n",
      "AUC under ROC for high inclusion exons: 0.6597391065980167\n",
      "epoch : 12\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.6587 - acc: 0.6173 - val_loss: 0.6360 - val_acc: 0.6937\n",
      "AUC under ROC for high inclusion exons: 0.6637764588100686\n",
      "epoch : 13\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 75us/step - loss: 0.6563 - acc: 0.6212 - val_loss: 0.6448 - val_acc: 0.6723\n",
      "AUC under ROC for high inclusion exons: 0.6695836908848207\n",
      "epoch : 14\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.6527 - acc: 0.6253 - val_loss: 0.6700 - val_acc: 0.6002\n",
      "AUC under ROC for high inclusion exons: 0.6757931683829138\n",
      "epoch : 15\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.6479 - acc: 0.6331 - val_loss: 0.6442 - val_acc: 0.6659\n",
      "AUC under ROC for high inclusion exons: 0.6821456664759726\n",
      "epoch : 16\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.6455 - acc: 0.6372 - val_loss: 0.6491 - val_acc: 0.6505\n",
      "AUC under ROC for high inclusion exons: 0.6885458381006866\n",
      "epoch : 17\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.6407 - acc: 0.6429 - val_loss: 0.6330 - val_acc: 0.6862\n",
      "AUC under ROC for high inclusion exons: 0.694084906559878\n",
      "epoch : 18\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.6377 - acc: 0.6452 - val_loss: 0.6385 - val_acc: 0.6655\n",
      "AUC under ROC for high inclusion exons: 0.6994541380625476\n",
      "epoch : 19\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.6327 - acc: 0.6527 - val_loss: 0.6440 - val_acc: 0.6468\n",
      "AUC under ROC for high inclusion exons: 0.7044032465675056\n",
      "epoch : 20\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.6306 - acc: 0.6536 - val_loss: 0.5869 - val_acc: 0.7661\n",
      "AUC under ROC for high inclusion exons: 0.7069537805110604\n",
      "epoch : 21\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.6255 - acc: 0.6593 - val_loss: 0.6304 - val_acc: 0.6700\n",
      "AUC under ROC for high inclusion exons: 0.7114827660183066\n",
      "epoch : 22\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.6223 - acc: 0.6591 - val_loss: 0.6057 - val_acc: 0.7158\n",
      "AUC under ROC for high inclusion exons: 0.71328542143402\n",
      "epoch : 23\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.6193 - acc: 0.6645 - val_loss: 0.5989 - val_acc: 0.7286\n",
      "AUC under ROC for high inclusion exons: 0.7161994660564455\n",
      "epoch : 24\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 75us/step - loss: 0.6169 - acc: 0.6674 - val_loss: 0.6063 - val_acc: 0.7091\n",
      "AUC under ROC for high inclusion exons: 0.7184103260869565\n",
      "epoch : 25\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.6138 - acc: 0.6689 - val_loss: 0.6232 - val_acc: 0.6757\n",
      "AUC under ROC for high inclusion exons: 0.7222838005339436\n",
      "epoch : 26\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.6099 - acc: 0.6715 - val_loss: 0.6185 - val_acc: 0.6809\n",
      "AUC under ROC for high inclusion exons: 0.723824251525553\n",
      "epoch : 27\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.6078 - acc: 0.6752 - val_loss: 0.6154 - val_acc: 0.6847\n",
      "AUC under ROC for high inclusion exons: 0.7254630291762013\n",
      "epoch : 28\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.6057 - acc: 0.6765 - val_loss: 0.6216 - val_acc: 0.6764\n",
      "AUC under ROC for high inclusion exons: 0.7281565837147215\n",
      "epoch : 29\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.6037 - acc: 0.6760 - val_loss: 0.5937 - val_acc: 0.7158\n",
      "AUC under ROC for high inclusion exons: 0.7274176439740656\n",
      "epoch : 30\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.6017 - acc: 0.6784 - val_loss: 0.5941 - val_acc: 0.7166\n",
      "AUC under ROC for high inclusion exons: 0.7305670766590389\n",
      "epoch : 31\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.6003 - acc: 0.6796 - val_loss: 0.6049 - val_acc: 0.7012\n",
      "AUC under ROC for high inclusion exons: 0.7329120184973302\n",
      "epoch : 32\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5987 - acc: 0.6792 - val_loss: 0.5802 - val_acc: 0.7361\n",
      "AUC under ROC for high inclusion exons: 0.7336420194508009\n",
      "epoch : 33\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5959 - acc: 0.6817 - val_loss: 0.5650 - val_acc: 0.7613\n",
      "AUC under ROC for high inclusion exons: 0.7339608361937453\n",
      "epoch : 34\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.5933 - acc: 0.6836 - val_loss: 0.5992 - val_acc: 0.7087\n",
      "AUC under ROC for high inclusion exons: 0.7361508390541571\n",
      "epoch : 35\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5938 - acc: 0.6854 - val_loss: 0.5725 - val_acc: 0.7451\n",
      "AUC under ROC for high inclusion exons: 0.7357396548436307\n",
      "epoch : 36\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5882 - acc: 0.6888 - val_loss: 0.5725 - val_acc: 0.7429\n",
      "AUC under ROC for high inclusion exons: 0.7361806350114417\n",
      "epoch : 37\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5864 - acc: 0.6880 - val_loss: 0.5662 - val_acc: 0.7553\n",
      "AUC under ROC for high inclusion exons: 0.7367795337528604\n",
      "epoch : 38\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5846 - acc: 0.6923 - val_loss: 0.5776 - val_acc: 0.7354\n",
      "AUC under ROC for high inclusion exons: 0.7378939025553013\n",
      "epoch : 39\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5829 - acc: 0.6921 - val_loss: 0.5498 - val_acc: 0.7752\n",
      "AUC under ROC for high inclusion exons: 0.7374618611746758\n",
      "epoch : 40\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5801 - acc: 0.6945 - val_loss: 0.5585 - val_acc: 0.7609\n",
      "AUC under ROC for high inclusion exons: 0.7390082713577422\n",
      "epoch : 41\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5780 - acc: 0.6932 - val_loss: 0.5718 - val_acc: 0.7395\n",
      "AUC under ROC for high inclusion exons: 0.7398127622044242\n",
      "epoch : 42\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5770 - acc: 0.6968 - val_loss: 0.5618 - val_acc: 0.7530\n",
      "AUC under ROC for high inclusion exons: 0.7391632103356217\n",
      "epoch : 43\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5759 - acc: 0.6933 - val_loss: 0.5439 - val_acc: 0.7819\n",
      "AUC under ROC for high inclusion exons: 0.7395237414187643\n",
      "epoch : 44\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5746 - acc: 0.6967 - val_loss: 0.5455 - val_acc: 0.7748\n",
      "AUC under ROC for high inclusion exons: 0.7397591294813122\n",
      "epoch : 45\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5738 - acc: 0.6990 - val_loss: 0.5547 - val_acc: 0.7654\n",
      "AUC under ROC for high inclusion exons: 0.741350233600305\n",
      "epoch : 46\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.5711 - acc: 0.7001 - val_loss: 0.5399 - val_acc: 0.7767\n",
      "AUC under ROC for high inclusion exons: 0.7397412519069412\n",
      "epoch : 47\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5698 - acc: 0.7000 - val_loss: 0.5517 - val_acc: 0.7639\n",
      "AUC under ROC for high inclusion exons: 0.7425748474446987\n",
      "epoch : 48\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5685 - acc: 0.6995 - val_loss: 0.5471 - val_acc: 0.7684\n",
      "AUC under ROC for high inclusion exons: 0.7421338672768878\n",
      "epoch : 49\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5678 - acc: 0.7022 - val_loss: 0.5467 - val_acc: 0.7684\n",
      "AUC under ROC for high inclusion exons: 0.7423394593821511\n",
      "epoch : 50\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5644 - acc: 0.7027 - val_loss: 0.5605 - val_acc: 0.7504\n",
      "AUC under ROC for high inclusion exons: 0.7436028079710145\n",
      "epoch : 51\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5643 - acc: 0.7027 - val_loss: 0.5305 - val_acc: 0.7842\n",
      "AUC under ROC for high inclusion exons: 0.7414843154080855\n",
      "epoch : 52\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.5623 - acc: 0.7052 - val_loss: 0.5415 - val_acc: 0.7699\n",
      "AUC under ROC for high inclusion exons: 0.7435461956521738\n",
      "epoch : 53\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5616 - acc: 0.7042 - val_loss: 0.5262 - val_acc: 0.7909\n",
      "AUC under ROC for high inclusion exons: 0.7432154605263157\n",
      "epoch : 54\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5602 - acc: 0.7061 - val_loss: 0.5341 - val_acc: 0.7800\n",
      "AUC under ROC for high inclusion exons: 0.7433316647597255\n",
      "epoch : 55\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5606 - acc: 0.7051 - val_loss: 0.5457 - val_acc: 0.7631\n",
      "AUC under ROC for high inclusion exons: 0.7444117682112892\n",
      "epoch : 56\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5599 - acc: 0.7067 - val_loss: 0.5219 - val_acc: 0.7917\n",
      "AUC under ROC for high inclusion exons: 0.7437488081617087\n",
      "epoch : 57\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5567 - acc: 0.7081 - val_loss: 0.5352 - val_acc: 0.7748\n",
      "AUC under ROC for high inclusion exons: 0.7457540760869565\n",
      "epoch : 58\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5585 - acc: 0.7058 - val_loss: 0.5585 - val_acc: 0.7462\n",
      "AUC under ROC for high inclusion exons: 0.7459298722349351\n",
      "epoch : 59\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5557 - acc: 0.7081 - val_loss: 0.5061 - val_acc: 0.8138\n",
      "AUC under ROC for high inclusion exons: 0.7448438095919145\n",
      "epoch : 60\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5525 - acc: 0.7103 - val_loss: 0.5403 - val_acc: 0.7703\n",
      "AUC under ROC for high inclusion exons: 0.7468297101449276\n",
      "epoch : 61\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5538 - acc: 0.7101 - val_loss: 0.5103 - val_acc: 0.8056\n",
      "AUC under ROC for high inclusion exons: 0.7461652602974829\n",
      "epoch : 62\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5511 - acc: 0.7131 - val_loss: 0.4916 - val_acc: 0.8262\n",
      "AUC under ROC for high inclusion exons: 0.7467522406559878\n",
      "epoch : 63\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5513 - acc: 0.7128 - val_loss: 0.5038 - val_acc: 0.8108\n",
      "AUC under ROC for high inclusion exons: 0.7474345680778032\n",
      "epoch : 64\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5495 - acc: 0.7102 - val_loss: 0.5187 - val_acc: 0.7932\n",
      "AUC under ROC for high inclusion exons: 0.748060283180778\n",
      "epoch : 65\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5481 - acc: 0.7145 - val_loss: 0.5192 - val_acc: 0.7924\n",
      "AUC under ROC for high inclusion exons: 0.7486204471777269\n",
      "epoch : 66\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 63us/step - loss: 0.5493 - acc: 0.7117 - val_loss: 0.5142 - val_acc: 0.8003\n",
      "AUC under ROC for high inclusion exons: 0.7499865918192219\n",
      "epoch : 67\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 74us/step - loss: 0.5476 - acc: 0.7152 - val_loss: 0.5026 - val_acc: 0.8104\n",
      "AUC under ROC for high inclusion exons: 0.7497109792143402\n",
      "epoch : 68\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5472 - acc: 0.7168 - val_loss: 0.5028 - val_acc: 0.8112\n",
      "AUC under ROC for high inclusion exons: 0.7506942458047292\n",
      "epoch : 69\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5441 - acc: 0.7159 - val_loss: 0.5007 - val_acc: 0.8123\n",
      "AUC under ROC for high inclusion exons: 0.7515881245232647\n",
      "epoch : 70\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 75us/step - loss: 0.5447 - acc: 0.7179 - val_loss: 0.5021 - val_acc: 0.8101\n",
      "AUC under ROC for high inclusion exons: 0.7517743492562928\n",
      "epoch : 71\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5427 - acc: 0.7179 - val_loss: 0.5128 - val_acc: 0.7924\n",
      "AUC under ROC for high inclusion exons: 0.7519158800533944\n",
      "epoch : 72\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5441 - acc: 0.7168 - val_loss: 0.5030 - val_acc: 0.8086\n",
      "AUC under ROC for high inclusion exons: 0.7520171863081617\n",
      "epoch : 73\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5422 - acc: 0.7178 - val_loss: 0.4980 - val_acc: 0.8149\n",
      "AUC under ROC for high inclusion exons: 0.7511054300152555\n",
      "epoch : 74\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5444 - acc: 0.7155 - val_loss: 0.4952 - val_acc: 0.8194\n",
      "AUC under ROC for high inclusion exons: 0.7524581664759725\n",
      "epoch : 75\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5403 - acc: 0.7187 - val_loss: 0.4906 - val_acc: 0.8217\n",
      "AUC under ROC for high inclusion exons: 0.7529825753241799\n",
      "epoch : 76\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5398 - acc: 0.7213 - val_loss: 0.5002 - val_acc: 0.8071\n",
      "AUC under ROC for high inclusion exons: 0.7523657990083905\n",
      "epoch : 77\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5391 - acc: 0.7188 - val_loss: 0.4790 - val_acc: 0.8352\n",
      "AUC under ROC for high inclusion exons: 0.752917024218154\n",
      "epoch : 78\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5395 - acc: 0.7194 - val_loss: 0.5015 - val_acc: 0.8067\n",
      "AUC under ROC for high inclusion exons: 0.7541893115942029\n",
      "epoch : 79\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5382 - acc: 0.7209 - val_loss: 0.4845 - val_acc: 0.8270\n",
      "AUC under ROC for high inclusion exons: 0.7530004528985508\n",
      "epoch : 80\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 76us/step - loss: 0.5364 - acc: 0.7188 - val_loss: 0.4912 - val_acc: 0.8176\n",
      "AUC under ROC for high inclusion exons: 0.7537423722349351\n",
      "epoch : 81\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.5379 - acc: 0.7211 - val_loss: 0.4989 - val_acc: 0.8082\n",
      "AUC under ROC for high inclusion exons: 0.7552142925247902\n",
      "epoch : 82\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5371 - acc: 0.7193 - val_loss: 0.4923 - val_acc: 0.8142\n",
      "AUC under ROC for high inclusion exons: 0.7538228213196034\n",
      "epoch : 83\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5369 - acc: 0.7206 - val_loss: 0.5028 - val_acc: 0.8026\n",
      "AUC under ROC for high inclusion exons: 0.7570973970251716\n",
      "epoch : 84\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5352 - acc: 0.7221 - val_loss: 0.4953 - val_acc: 0.8134\n",
      "AUC under ROC for high inclusion exons: 0.7554884153318078\n",
      "epoch : 85\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5347 - acc: 0.7240 - val_loss: 0.4981 - val_acc: 0.8026\n",
      "AUC under ROC for high inclusion exons: 0.7571689073226544\n",
      "epoch : 86\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5347 - acc: 0.7232 - val_loss: 0.4892 - val_acc: 0.8157\n",
      "AUC under ROC for high inclusion exons: 0.7554049866514112\n",
      "epoch : 87\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5341 - acc: 0.7231 - val_loss: 0.5090 - val_acc: 0.7860\n",
      "AUC under ROC for high inclusion exons: 0.7579644593821511\n",
      "epoch : 88\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5338 - acc: 0.7217 - val_loss: 0.5015 - val_acc: 0.7984\n",
      "AUC under ROC for high inclusion exons: 0.7582653985507247\n",
      "epoch : 89\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5324 - acc: 0.7244 - val_loss: 0.4843 - val_acc: 0.8247\n",
      "AUC under ROC for high inclusion exons: 0.7571867848970252\n",
      "epoch : 90\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5323 - acc: 0.7226 - val_loss: 0.4919 - val_acc: 0.8172\n",
      "AUC under ROC for high inclusion exons: 0.7581432351258581\n",
      "epoch : 91\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5298 - acc: 0.7283 - val_loss: 0.4829 - val_acc: 0.8251\n",
      "AUC under ROC for high inclusion exons: 0.7576516018306636\n",
      "epoch : 92\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5300 - acc: 0.7255 - val_loss: 0.4760 - val_acc: 0.8311\n",
      "AUC under ROC for high inclusion exons: 0.7585365417620138\n",
      "epoch : 93\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5304 - acc: 0.7241 - val_loss: 0.4991 - val_acc: 0.8011\n",
      "AUC under ROC for high inclusion exons: 0.759046052631579\n",
      "epoch : 94\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5305 - acc: 0.7276 - val_loss: 0.4898 - val_acc: 0.8116\n",
      "AUC under ROC for high inclusion exons: 0.7598326659038903\n",
      "epoch : 95\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5323 - acc: 0.7256 - val_loss: 0.4909 - val_acc: 0.8127\n",
      "AUC under ROC for high inclusion exons: 0.7596509105644546\n",
      "epoch : 96\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42888/42888 [==============================] - 3s 63us/step - loss: 0.5283 - acc: 0.7276 - val_loss: 0.4731 - val_acc: 0.8300\n",
      "AUC under ROC for high inclusion exons: 0.7598147883295194\n",
      "epoch : 97\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 61us/step - loss: 0.5278 - acc: 0.7283 - val_loss: 0.4913 - val_acc: 0.8059\n",
      "AUC under ROC for high inclusion exons: 0.760088911136537\n",
      "epoch : 98\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5269 - acc: 0.7287 - val_loss: 0.4816 - val_acc: 0.8221\n",
      "AUC under ROC for high inclusion exons: 0.7597641352021357\n",
      "epoch : 99\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5288 - acc: 0.7267 - val_loss: 0.4819 - val_acc: 0.8213\n",
      "AUC under ROC for high inclusion exons: 0.7594959715865751\n",
      "epoch : 100\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 63us/step - loss: 0.5273 - acc: 0.7300 - val_loss: 0.4658 - val_acc: 0.8356\n",
      "AUC under ROC for high inclusion exons: 0.7595645022883294\n",
      "epoch : 101\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5262 - acc: 0.7277 - val_loss: 0.4627 - val_acc: 0.8397\n",
      "AUC under ROC for high inclusion exons: 0.758861317696415\n",
      "epoch : 102\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5255 - acc: 0.7300 - val_loss: 0.4865 - val_acc: 0.8131\n",
      "AUC under ROC for high inclusion exons: 0.7603153604118994\n",
      "epoch : 103\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5272 - acc: 0.7268 - val_loss: 0.4861 - val_acc: 0.8112\n",
      "AUC under ROC for high inclusion exons: 0.7602647072845157\n",
      "epoch : 104\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5246 - acc: 0.7309 - val_loss: 0.4973 - val_acc: 0.7969\n",
      "AUC under ROC for high inclusion exons: 0.7599011966056445\n",
      "epoch : 105\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 75us/step - loss: 0.5236 - acc: 0.7323 - val_loss: 0.4834 - val_acc: 0.8119\n",
      "AUC under ROC for high inclusion exons: 0.7596360125858124\n",
      "epoch : 106\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5248 - acc: 0.7273 - val_loss: 0.4881 - val_acc: 0.8078\n",
      "AUC under ROC for high inclusion exons: 0.7599071557971016\n",
      "epoch : 107\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5283 - acc: 0.7297 - val_loss: 0.4814 - val_acc: 0.8119\n",
      "AUC under ROC for high inclusion exons: 0.7584620518688026\n",
      "epoch : 108\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5246 - acc: 0.7295 - val_loss: 0.4732 - val_acc: 0.8243\n",
      "AUC under ROC for high inclusion exons: 0.7610394021739131\n",
      "epoch : 109\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5226 - acc: 0.7338 - val_loss: 0.4874 - val_acc: 0.8078\n",
      "AUC under ROC for high inclusion exons: 0.7610662185354691\n",
      "epoch : 110\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5236 - acc: 0.7324 - val_loss: 0.4583 - val_acc: 0.8393\n",
      "AUC under ROC for high inclusion exons: 0.7609172387490466\n",
      "epoch : 111\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5220 - acc: 0.7311 - val_loss: 0.4596 - val_acc: 0.8367\n",
      "AUC under ROC for high inclusion exons: 0.7586974399313502\n",
      "epoch : 112\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5225 - acc: 0.7298 - val_loss: 0.4853 - val_acc: 0.8086\n",
      "AUC under ROC for high inclusion exons: 0.7609559734935164\n",
      "epoch : 113\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5223 - acc: 0.7305 - val_loss: 0.4619 - val_acc: 0.8330\n",
      "AUC under ROC for high inclusion exons: 0.7589953995041953\n",
      "epoch : 114\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5216 - acc: 0.7348 - val_loss: 0.4679 - val_acc: 0.8232\n",
      "AUC under ROC for high inclusion exons: 0.7596121758199847\n",
      "epoch : 115\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5223 - acc: 0.7321 - val_loss: 0.4806 - val_acc: 0.8123\n",
      "AUC under ROC for high inclusion exons: 0.7590251954614797\n",
      "epoch : 116\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5220 - acc: 0.7337 - val_loss: 0.4594 - val_acc: 0.8360\n",
      "AUC under ROC for high inclusion exons: 0.7591741752479023\n",
      "epoch : 117\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5217 - acc: 0.7326 - val_loss: 0.4798 - val_acc: 0.8119\n",
      "AUC under ROC for high inclusion exons: 0.7592799508962624\n",
      "epoch : 118\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5216 - acc: 0.7329 - val_loss: 0.4719 - val_acc: 0.8179\n",
      "AUC under ROC for high inclusion exons: 0.7600144212433256\n",
      "epoch : 119\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5218 - acc: 0.7284 - val_loss: 0.4861 - val_acc: 0.8071\n",
      "AUC under ROC for high inclusion exons: 0.7605239321128908\n",
      "epoch : 120\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.5187 - acc: 0.7334 - val_loss: 0.4821 - val_acc: 0.8078\n",
      "AUC under ROC for high inclusion exons: 0.7602945032418\n",
      "epoch : 121\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5206 - acc: 0.7317 - val_loss: 0.4750 - val_acc: 0.8131\n",
      "AUC under ROC for high inclusion exons: 0.7606729118993134\n",
      "epoch : 122\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5197 - acc: 0.7329 - val_loss: 0.4892 - val_acc: 0.7980\n",
      "AUC under ROC for high inclusion exons: 0.7622789139969488\n",
      "epoch : 123\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5192 - acc: 0.7350 - val_loss: 0.4686 - val_acc: 0.8198\n",
      "AUC under ROC for high inclusion exons: 0.7606907894736843\n",
      "epoch : 124\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 71us/step - loss: 0.5179 - acc: 0.7352 - val_loss: 0.4554 - val_acc: 0.8371\n",
      "AUC under ROC for high inclusion exons: 0.7590251954614797\n",
      "epoch : 125\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5186 - acc: 0.7344 - val_loss: 0.4759 - val_acc: 0.8116\n",
      "AUC under ROC for high inclusion exons: 0.7590788281845919\n",
      "epoch : 126\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5167 - acc: 0.7351 - val_loss: 0.4779 - val_acc: 0.8104\n",
      "AUC under ROC for high inclusion exons: 0.761078136918383\n",
      "epoch : 127\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 64us/step - loss: 0.5173 - acc: 0.7347 - val_loss: 0.4710 - val_acc: 0.8146\n",
      "AUC under ROC for high inclusion exons: 0.7595168287566743\n",
      "epoch : 128\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5192 - acc: 0.7341 - val_loss: 0.4589 - val_acc: 0.8300\n",
      "AUC under ROC for high inclusion exons: 0.7600322988176964\n",
      "epoch : 129\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5179 - acc: 0.7349 - val_loss: 0.4759 - val_acc: 0.8093\n",
      "AUC under ROC for high inclusion exons: 0.7605030749427917\n",
      "epoch : 130\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5159 - acc: 0.7356 - val_loss: 0.4623 - val_acc: 0.8236\n",
      "AUC under ROC for high inclusion exons: 0.7601306254767354\n",
      "epoch : 131\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5174 - acc: 0.7370 - val_loss: 0.4814 - val_acc: 0.8044\n",
      "AUC under ROC for high inclusion exons: 0.7602170337528604\n",
      "epoch : 132\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 76us/step - loss: 0.5160 - acc: 0.7351 - val_loss: 0.4661 - val_acc: 0.8176\n",
      "AUC under ROC for high inclusion exons: 0.7593797673531655\n",
      "epoch : 133\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5144 - acc: 0.7382 - val_loss: 0.4737 - val_acc: 0.8093\n",
      "AUC under ROC for high inclusion exons: 0.75919801201373\n",
      "epoch : 134\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 72us/step - loss: 0.5143 - acc: 0.7385 - val_loss: 0.4816 - val_acc: 0.7999\n",
      "AUC under ROC for high inclusion exons: 0.7580031941266209\n",
      "epoch : 135\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5146 - acc: 0.7382 - val_loss: 0.4798 - val_acc: 0.8014\n",
      "AUC under ROC for high inclusion exons: 0.7592099303966439\n",
      "epoch : 136\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5144 - acc: 0.7389 - val_loss: 0.4836 - val_acc: 0.7984\n",
      "AUC under ROC for high inclusion exons: 0.7609291571319603\n",
      "epoch : 137\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5136 - acc: 0.7382 - val_loss: 0.4850 - val_acc: 0.7969\n",
      "AUC under ROC for high inclusion exons: 0.7601514826468345\n",
      "epoch : 138\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 61us/step - loss: 0.5123 - acc: 0.7388 - val_loss: 0.4698 - val_acc: 0.8097\n",
      "AUC under ROC for high inclusion exons: 0.7610632389397407\n",
      "epoch : 139\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5131 - acc: 0.7387 - val_loss: 0.4674 - val_acc: 0.8134\n",
      "AUC under ROC for high inclusion exons: 0.7590669098016782\n",
      "epoch : 140\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 73us/step - loss: 0.5141 - acc: 0.7381 - val_loss: 0.4671 - val_acc: 0.8134\n",
      "AUC under ROC for high inclusion exons: 0.7619064645308924\n",
      "epoch : 141\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 66us/step - loss: 0.5140 - acc: 0.7393 - val_loss: 0.4708 - val_acc: 0.8116\n",
      "AUC under ROC for high inclusion exons: 0.7618647501906941\n",
      "epoch : 142\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5110 - acc: 0.7428 - val_loss: 0.4457 - val_acc: 0.8356\n",
      "AUC under ROC for high inclusion exons: 0.7610691981311974\n",
      "epoch : 143\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5110 - acc: 0.7382 - val_loss: 0.4784 - val_acc: 0.7995\n",
      "AUC under ROC for high inclusion exons: 0.7621835669336383\n",
      "epoch : 144\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 70us/step - loss: 0.5118 - acc: 0.7382 - val_loss: 0.4698 - val_acc: 0.8086\n",
      "AUC under ROC for high inclusion exons: 0.7607146262395119\n",
      "epoch : 145\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 65us/step - loss: 0.5118 - acc: 0.7407 - val_loss: 0.4680 - val_acc: 0.8116\n",
      "AUC under ROC for high inclusion exons: 0.7608189120900076\n",
      "epoch : 146\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 69us/step - loss: 0.5128 - acc: 0.7393 - val_loss: 0.4921 - val_acc: 0.7905\n",
      "AUC under ROC for high inclusion exons: 0.7613939740655988\n",
      "epoch : 147\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 67us/step - loss: 0.5108 - acc: 0.7399 - val_loss: 0.4717 - val_acc: 0.8063\n",
      "AUC under ROC for high inclusion exons: 0.76150719870328\n",
      "epoch : 148\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 68us/step - loss: 0.5114 - acc: 0.7390 - val_loss: 0.4942 - val_acc: 0.7845\n",
      "AUC under ROC for high inclusion exons: 0.7621358934019833\n",
      "epoch : 149\n",
      "Train on 42888 samples, validate on 2664 samples\n",
      "Epoch 1/1\n",
      "42888/42888 [==============================] - 3s 61us/step - loss: 0.5114 - acc: 0.7408 - val_loss: 0.5057 - val_acc: 0.7733\n",
      "AUC under ROC for high inclusion exons: 0.7614505863844394\n"
     ]
    }
   ],
   "source": [
    "for i in range (150):\n",
    "    \n",
    "\n",
    "    \n",
    "    print('epoch :',i)\n",
    "    model.fit([train[:,:l-1,:4],train[:,-1,0:3]], y_train,validation_data=([test[:,:l-1,:4],test[:,-1,0:3]], y_test), epochs = 1, verbose = 1, batch_size = 126)\n",
    "    y_=model.predict([htest[:,:l-1,:4],htest[:,-1,0:3]])\n",
    "    print(\"AUC under ROC for high inclusion exons:\",roc_auc_score(hy_test, y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7614505863844394, 0.8660589305711086, 0.803835001701452)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_=model.predict([htest[:,:l-1,:4],htest[:,-1,0:3]])\n",
    "ac1 = roc_auc_score(hy_test, y1_)\n",
    "\n",
    "y2_=model.predict([ltest[:,:l-1,:4],ltest[:,-1,0:3]])\n",
    "ac3 = roc_auc_score(ly_test, y2_)\n",
    "\n",
    "y_=model.predict([test[:,:l-1,:4],test[:,-1,0:3]])\n",
    "ac2 = roc_auc_score(y_test, y_)\n",
    "ac1,ac3,ac2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
